<h1>자기 주도 온라인 프로젝트</h1>

`>` 인공지능, 빅데이터, 블록체인 순서로 작성

<br/>

<h2>인공지능 (1) ~ (4)</h2>

> 인공지능 : 

<br/>

AI : 사람의 지능 

머신 러닝 (ML) : 비정형 데이터를 분석, AI의 일부분

빅데이터 : 데이터가 많다. 해석, 분석, 이해하기 위한 방법론

데니터 마이닝 : 데이터 자체의 차이, 대부분 정형 데이터

통계학 : 연관이 되지만 훨씬 더 많고 다양한 데이터를 사용

<br/>

**training → 컴퓨터 → testing**

<br/>

선형모델 : 입력 특성에 대한 선형 함수를 만들어 예측을 수행

비선형 모델 : 식변형으로 변수들을 선형 결합화 시킬 수 없는 모델

<br/>

**<대표 모델>**

* IBM Watson Jeopardy : 퀴즈

* AlphaGo Zero
* Google Duplex : 챗봇

<br/>

**Regression**

* 값을 내주는 것, 예측하는 데 도움을 줌

* Linear Regression : 선을 그려 x 값에 대한 y의 예측 값을 알려준다
  $$
  y = W^T*x + e
  $$

* Ridge Regression : 패널티(에러 제거)를 줘 스무스한 값(이상적인 값)을 찾아준다

<br/>

**Naive Bayes Classifier**

* 모형 자체가 말이 되는 모형
* 많은 머신 러닝 연구, 응용 상황에서 기본 모형으로 사용 (결과는 그렇게 좋진 않음)
* features : 성질, data detail, image classifier
* feature를 보고 결과를 예측 (이 사람이 결혼을 했을까?)
* simple classification : two binary features
* Digit Recognizer : 픽셀로 나눠 값을 유추 (ex. 3)
* Text mining에서 자주 사용
* 조건부 확률
* feature 들이 독립적이다. 따로 존재 

<br/>

**Smoothing**

* 데이터를 held-out (validation)
* baseline 과 비교
* 대부분 0, 1의 확률 값이 아닌 RGB real-valued functions 
* 모델이 새로운 데이터에 대해서도 잘 예측할 수 있도록 해준다

<br/>

**Deep Natural Language Processing** (NLP)

* Word Vector Representations : 하나의 단어를 실수 여러개로 표현
* 비슷한 단어가 근처에 있음을 유추할 수 있어 벡터로 표현 (cat, cats)
* Distributed Representations : 여러 뉴런의 컨셉으로 관계를 방향성으로 표현
* 문법적인 것도 표현 할 수 있다
* 문맥에 따라서 단어의 뜻을 유추할 수 있다
* Word2Vec : 어떤 방식으로 추론을 하는지 (CBOW-처음, Skip-Gram-중간)

<br/>

**결과 확인 방법**

* Word Similarity Task : 사람들이 봤을 때의 단어들의 유사도 수치와 비교
* Word Analogy Task : A:B = C:D 하나를 지웠을 때 관계를 얼마나 잘 찾아내는 지

<br/>

**문제**

* Out-of-vocabularies : 넣지 않은 단어는 확인 못함
* 많이 안나온 단어는 학습을 해도 결과가 좋지 않다
* 미인이라는 단어는 예쁜 + 사람인데 유추를 못할 수도 있다

<br/>

**ELMo**

* I left my room left ~~ → 처음 left 남겨두다 두번째 left 왼쪽
* 각각의 단어가 독립적으로 계산되는 것이 아니라 정보를 가지고 값을 유추함
* 단어가 가지고 있는 의미를 어떤 문맥에서 나왔는지 캡처
* 문제 : 문장이 길어질 수록 정보를 많이 가지고 있어서 처음에 가지고 있는 의미를 잃는다, 가까운 단어만 가지고 있는데 가까운 단어가 무조건 관계가 있는 게 아니다
* 한 쪽 방향성만 가진다

<br/>

**BERT**

* Transformers : 순서에 따라가 아니라 단어에 따라 연관성을 가진다
* 멀리있어도 연관성을 캡처하고 방향성을 양쪽으로 가진다
* Position Enbedding
* 두개의 문장에 대한 연관성
* Tagging 작업을 할 떄 많이 사용 된다

<br/><br/>

[참고 자료]

https://jaehyeongan.github.io/2019/04/25/Linear-Regression/ <br/>https://blog.naver.com/kh9342/221780884161 <br/>https://blog.naver.com/ollehw/221683331361<br/><br/><br/>

<h2>빅데이터 (1) ~ (2)</h2>

> 빅데이터 : 

<br/>

**Clustering** 

* 데이터를 유사도에 의해 k개로 그룹을 나누는 것
* 추천 시스템에 많이 사용
* 여러 가지 클러스터링 결과가 나올 수 있다 n개의 부분 집합의 개수 만큼 가능 (2^n)
* 따라서 전부 다 해보려면 많이 사간이 소요된다
* 각각의 클러스터 마다 평균이 되는 차이를 제곱해서 구해 비슷한 그룹을 구한다

* Partional Algorithms : 전부 다 계산해보는 것이 아니라 근사적으로 일부 값만 보고 비슷한 그룹을 구함

<br/>

**K-Means Clustering**

* 랜덤하게 그룹을 나누고 평균점을 구한다
* 어느 점에 가까운지 정렬한다
* 다시 평균점을 구하고 정렬하다 변화가 없을 때 끝낸다
* size가 크거나 작을 때 못함, 공같은 모양일때만 잘함, outlier 때문에 실제 데이터가 없는 곳에서 평균점을 정할 때가 있을 수 있다

<br/>

**Hierarchial Clustering**

* 처음에 n개의 포인트가 있고 모든 점들의 거리를 구해 가장 가까운 점들을 1개씩 merge 해 가면서 k개가 되면 종료
* merge 한 점의 distance를 계산하는 방법에 따라 성능이 달라진다
* single-link, complete-link, average-link, mean-link, centroid-link 등이 있다
* 거리가 같을 경우 아무거나 merge
* centroid-link  : merge 한 후 중간점을 기준점으로 바꾸고 거리를 다시 구해 가까운 점 merge
* single-link : merge 한 후 중간점이 아닌 원래 점 기준으로 거리로 구함
* average-link : centroid-link 과 비슷?
* complete-link : max distance 비교
* 예시에서는 어떤 알고리즘을 써도 같은 결과가 나온다

<br/>

**DBSCAN Clustering**

* Eps-neighbors와 Minpts를 파라미터로 사용하여 군집을 구성하는 알고리즘
* Eps-neighbors란 한 데이터를 중심으로 epsilon거리 이내의 데이터를 한 군집으로 구성하는 걸 의미
* Minpts란 한 군집의 데이터 수 >= Minpts로 구성될 때 그 갯수를 의미
* 만약  Minpts보다 적은 수의 데이터가 Eps-neighbors를 구성하면 노이즈로 취급
* outlier 가 존재한다
* epsilon이 작을 수록 minPoints 가 클수록 dense (클러스트가 여러개 생성)한 알고리즘이다

<br/>

**EM Clustering**

*  **관측되지 않는 잠재변수**에 의존하는 확률 모델에서 **최대우도**나 **최대사후확률**을 갖는 **매개변수를 찾는 반복적인 알고리즘**
*  likelihood : 어떤 시행의 결과가 주어졌다 했을 때, 만약 주어진 가설 H가 참이라면, 그러한 결과 E가 나오는 확률
* 즉, 역으로 생성 모델을 찾아 나가겠다는 의미
* E(expectation)단계: E[logf(h,v;u)|V]를 구한다 (h는 hidden, v는 visual 변수)
* M(maximization)단계: E 단계에서 구한 것을 u에 관한 함수로 보고 최대가 되는 u를 구한다
* 이 두 단계를 어느 정도 수렴할 정도까지 계속 한다.

<br/><br/>

[참고 자료]

https://blog.naver.com/alstjddl8/221695579087<br/>https://blog.naver.com/gyeongbly_park/221828790261<br/>https://blog.naver.com/lhm0812/220709231513<br/>http://jaekwangkim.com/articles/2017-02/EM_Algorithm<br/>http://blog.daum.net/dataminer9/173<br/>

<br/><br/>

<h2>블록체인 (1) ~ (2)</h2>

> 블록체인 : 

<br/>



<br/><br/>

<h2>시청 소감</h2>

<br/>

* 인공지능

```
이해가 안됐습니다... 그래서 다른 자료들을 찾아가며 강의를 듣긴 했지만, 제대로 된 정리나 실습도 없이 계속 공식을 알려주다 보니 더 어렵게 느껴졌습니다.
정리를 꼼꼼하게 하게 싶었지만 자료가 다 영어로 되어 있고 수식이라 최대한 이해한 내용 위주로 정리했습니다. 
입문자용 강의가 아닌 거 같습니다 ㅠㅠ
```

<br/>

* 빅데이터

```
실제로 된 데이터는 어렵다고 간단한 수치로 예시를 들어 설명해줘 이해하기 쉬웠습니다.
수식을 몰라도 원리를 이해할 수 있게 쉽게 알려줘 나중에 자세하게 공부할 때 도움이 많이 될 것 같습니다.
DBSCAN, EM 클러스터링 알고리즘은 이해가 안되서 인터넷 보고 다시 공부하고 보니 이해가 됐습니다.
하루에 강의를 8시간정도 듣고, 취업특강까지 있어서 자료가 영어라 하나 하나 이해하면서 듣기에는 빠듯했습니다.
```

<br/>

* 블록체인

```

```

<br/><br/>

AI             ~~55~~ ~~26~~ ~~40~~ ~~42~~
BigD        ~~40~~ 58
BlockC    66 70